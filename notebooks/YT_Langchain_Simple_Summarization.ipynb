{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRYSu48huSUW",
        "outputId": "02733255-d061-4563-c8cf-f4212fc70715"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain openai==0.27.0 tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW6FD6FsT5Qf"
      },
      "source": [
        "# Summarization\n",
        "\n",
        "History  \n",
        "Challenges  \n",
        "Fine-tuning  \n",
        "Instruct Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#os.environ[\"OPENAI_API_BASE\"] = \"https://proxy.dta.totvs.ai/\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://proxy.dta.totvs.ai/\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-axyZ_tPhqNPbbywhdhhhKQ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-KFB7J_u_3L",
        "outputId": "5657944a-c8fe-48dd-d131-9a649d84b785"
      },
      "outputs": [],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "### Setting up Summarization Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "#llm = OpenAI(temperature=0)\n",
        "llm = OpenAI(\n",
        "            openai_api_base=\"https://proxy.dta.totvs.ai/\",\n",
        "            openai_api_key=\"sk-axyZ_tPhqNPbbywhdhhhKQ\",\n",
        "            temperature=0,\n",
        "            model=\"gpt-4-0125-preview\",\n",
        "        )\n",
        "\n",
        "print(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyPulL7tvQOw"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCfCSX9sOv2A"
      },
      "outputs": [],
      "source": [
        "# load the doc\n",
        "with open('sentence.txt') as f:\n",
        "    how_to_win_friends = f.read()\n",
        "texts = text_splitter.split_text(how_to_win_friends)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6KdLobvkLje",
        "outputId": "046af023-0ed4-406f-bfac-ca2fef9b86c3"
      },
      "outputs": [],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L6ztJUJO2j8"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts[:4]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlSA_nIxdrSU",
        "outputId": "d99b2de5-e591-4b6e-b6ac-d550bf1ef2c6"
      },
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IczFCIKaLCet"
      },
      "source": [
        "##  3 types of CombineDocuments Chains\n",
        "\n",
        "[Taken from the LangChain Docs](https://langchain.readthedocs.io/en/latest/modules/indexes/combine_docs.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6kbcja8O7-Q"
      },
      "source": [
        "## Summarize Simple with map_reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zr-AO5qlJNZ"
      },
      "source": [
        "### Map Reduce\n",
        "This method involves **an initial prompt on each chunk of data ***\n",
        "( for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk). **Then a different prompt is run to combine all the initial outputs.** This is implemented in the LangChain as the MapReduceDocumentsChain.\n",
        "\n",
        "**Pros:** Can scale to larger documents (and more documents) than StuffDocumentsChain. The calls to the LLM on individual documents are independent and can therefore be parallelized.\n",
        "\n",
        "**Cons:** Requires many more calls to the LLM than StuffDocumentsChain. Loses some information during the final combining call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 481,
      "metadata": {
        "id": "lFYz2IztO2nE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 482,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4nRP8oGO2qf",
        "outputId": "1b6f930e-6bf0-41b3-bee8-08409271ce22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Daniel Dias reached out to Sandra Regina Barsch from the Sales & CRM Support team about an issue\n",
            "where an order was initially disapproved but later partially billed, only to revert to disapproved\n",
            "due to a setting in the CM0101 configuration. He provided a screenshot for establishment 600,\n",
            "seeking clarification on why the order, blocked on credit on March 8th, proceeded with shipping and\n",
            "billing, and mentioned generating the spp of the order for analysis.\n"
          ]
        }
      ],
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\")\n",
        "\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 483,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k0OK53Q3fReD",
        "outputId": "37a72505-6980-4d5a-a269-a0968e1276e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ]
          },
          "execution_count": 483,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for summarizing each part\n",
        "chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L2JttqnWpR0H",
        "outputId": "03fe1f6d-16da-4cce-eb2d-247ebfb1de9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ]
          },
          "execution_count": 484,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for combining the parts\n",
        "chain.combine_document_chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for MapReduceDocumentsChain\nquestion_prompt\n  extra fields not permitted (type=value_error.extra)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[489], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWrite a concise summary, in chronological order and \u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m                     in the Brazilian Portuguese language \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m                     Please disregard personal names\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mCONCISE SUMMARY:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mprompt_template,\n\u001b[1;32m     12\u001b[0m                         input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_summarize_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmap_reduce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquestion_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# chain = load_summarize_chain(llm,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#                              chain_type=\"refine\",\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#                              return_intermediate_steps=True,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#                              question_prompt=PROMPT,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#                              refine_prompt=refine_prompt)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m output_summary \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mrun(docs)\n",
            "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/langchain/chains/summarize/__init__.py:166\u001b[0m, in \u001b[0;36mload_summarize_chain\u001b[0;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     )\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/langchain/chains/summarize/__init__.py:105\u001b[0m, in \u001b[0;36m_load_map_reduce_chain\u001b[0;34m(llm, map_prompt, combine_prompt, combine_document_variable_name, map_reduce_document_variable_name, collapse_prompt, reduce_llm, collapse_llm, verbose, token_max, callbacks, collapse_max_retries, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     collapse_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m     89\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mLLMChain(\n\u001b[1;32m     90\u001b[0m             llm\u001b[38;5;241m=\u001b[39m_collapse_llm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39mcombine_document_variable_name,\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     97\u001b[0m reduce_documents_chain \u001b[38;5;241m=\u001b[39m ReduceDocumentsChain(\n\u001b[1;32m     98\u001b[0m     combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain,\n\u001b[1;32m     99\u001b[0m     collapse_documents_chain\u001b[38;5;241m=\u001b[39mcollapse_chain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     collapse_max_retries\u001b[38;5;241m=\u001b[39mcollapse_max_retries,\n\u001b[1;32m    104\u001b[0m )\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMapReduceDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_documents_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_documents_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_reduce_document_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for MapReduceDocumentsChain\nquestion_prompt\n  extra fields not permitted (type=value_error.extra)"
          ]
        }
      ],
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\",\n",
        "                             verbose=True\n",
        "                             )\n",
        "\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6yiwXNnvzxO"
      },
      "source": [
        "### Summarizing with the 'stuff' Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o7FgrlMhuW2"
      },
      "source": [
        "\n",
        "\n",
        "### Stuffing\n",
        "Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n",
        "\n",
        "**Pros:** Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n",
        "\n",
        "**Cons:** Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
        "\n",
        "The main downside of this method is that **it only works one smaller pieces of data.**  Once you are working with many pieces of data, this approach is no longer feasible. The next two approaches are designed to help deal with that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzCHODNOPKnO"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTMPc6oQPQr0"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
        "\n",
        "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrtSb8VphVG-",
        "outputId": "8c59f9f3-9a3e-4c7a-a232-1dfc2394fde4"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=BULLET_POINT_PROMPT)\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ_ig4J_PYbm"
      },
      "source": [
        "### Ver 3 With 'map_reduce' with our custom prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lS7vYgKPcqV",
        "outputId": "e2824e48-6d27-4e1a-ba53-489574722574"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\",\n",
        "                             map_prompt=BULLET_POINT_PROMPT,\n",
        "                             combine_prompt=BULLET_POINT_PROMPT)\n",
        "\n",
        "# chain.llm_chain.prompt= BULLET_POINT_PROMPT\n",
        "# chain.combine_document_chain.llm_chain.prompt= BULLET_POINT_PROMPT\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUFsaodtPkl4",
        "outputId": "e49cc8b4-f081-43d8-b0b3-3257f1f6e5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Daniel's order was initially rejected but later approved for partial billing.\n",
            "- Post-billing, the\n",
            "order was again rejected due to the \"Reopen credit evaluation\" setting in CM0101.\n",
            "- Sandra requested\n",
            "a screenshot of the CM0101 screen from establishment 600 for analysis.\n",
            "- Daniel provided the\n",
            "requested screenshot and noted that despite a credit block on 08/03, the order proceeded to shipment\n",
            "and billing.\n",
            "- Daniel seeks help to understand the credit issue and has prepared the spp of the\n",
            "order for analysis.\n"
          ]
        }
      ],
      "source": [
        "# with a custom prompt\n",
        "prompt_template = \"\"\"Write a concise summary of the following:\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])\n",
        "\n",
        "## with intermediate steps\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\",\n",
        "                             return_intermediate_steps=True,\n",
        "                             map_prompt=PROMPT,\n",
        "                             combine_prompt=PROMPT)\n",
        "\n",
        "output_summary = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
        "wrapped_text = textwrap.fill(output_summary['output_text'],\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvksjoTytSCb",
        "outputId": "03bc2547-cb3c-45ad-e546-404993cf1f8f"
      },
      "outputs": [],
      "source": [
        "wrapped_text = textwrap.fill(output_summary['intermediate_steps'][2],\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caaAmomfPv9j"
      },
      "source": [
        "### With the 'refine' CombineDocument Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39P6zjy9lT5X"
      },
      "source": [
        "## Refine\n",
        "This method involves **an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document**, asking the LLM to refine the output based on the new document.\n",
        "\n",
        "**Pros:** Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
        "\n",
        "**Cons:** Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ji2D8Q0P2gh",
        "outputId": "cb493a3e-93f2-4fee-e6fc-4e4308e62aa4"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {
        "id": "JsoM2yyoQNzF"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Write a concise summary, in chronological order and \n",
        "                     in the Brazilian Portuguese language \n",
        "                     Please disregard personal names\n",
        "                     of the following:\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])\n",
        "\n",
        "refine_template = (\n",
        "    \"Your job is to produce a final summary\\n\"\n",
        "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing summary\"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{text}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original summary\"\n",
        "    \"If the context isn't useful, return the original summary.\"\n",
        ")\n",
        "\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"existing_answer\", \"text\"],\n",
        "    template=refine_template,\n",
        ")\n",
        "\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"refine\",\n",
        "                             return_intermediate_steps=True,\n",
        "                             question_prompt=PROMPT,\n",
        "                             refine_prompt=refine_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgIuOQWbfFep",
        "outputId": "6b701913-2b71-4652-cfa6-1e96f542ee20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inicialmente, um pedido foi reprovado, mas acabou sendo aprovado forçadamente para permitir o\n",
            "faturamento parcial. Após o faturamento, o pedido foi novamente reprovado devido à configuração do\n",
            "campo \"Reabre avaliação de crédito\" no CM0101. Sandra Regina Barsch, do Suporte Vendas & CRM,\n",
            "menciona a documentação relevante sobre o campo e solicita esclarecimentos. Daniel Dias, por sua\n",
            "vez, envia um print da tela CM0101 do estabelecimento 600 e relata que, no dia 08/03, apesar do\n",
            "bloqueio de crédito, foi possível prosseguir com o embarque e faturamento do pedido. Ele solicita\n",
            "análise para entender o ocorrido com o crédito.\n"
          ]
        }
      ],
      "source": [
        "output_summary = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
        "wrapped_text = textwrap.fill(output_summary['output_text'],\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt7v2DVavMpX",
        "outputId": "eddfab8d-2731-492f-fc16-8d8c25d2aec8"
      },
      "outputs": [],
      "source": [
        "wrapped_text = textwrap.fill(output_summary['intermediate_steps'][0],\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtEBpGcCRGzk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
