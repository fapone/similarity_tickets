{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e217a187-9026-4173-9a69-8c46708f8227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdb_dtypes\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mftfy\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/similarity-tickets/notebooks/.venv/lib/python3.12/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hard_dependencies, dependency, missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     np_version_under1p18 \u001b[38;5;28;01mas\u001b[39;00m _np_version_under1p18,\n\u001b[1;32m     24\u001b[0m     is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/similarity-tickets/notebooks/.venv/lib/python3.12/site-packages/pandas/compat/__init__.py:23\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     is_numpy_dev,\n\u001b[1;32m     17\u001b[0m     np_array_datetime64_compat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     np_version_under1p20,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     pa_version_under1p0,\n\u001b[1;32m     25\u001b[0m     pa_version_under2p0,\n\u001b[1;32m     26\u001b[0m     pa_version_under3p0,\n\u001b[1;32m     27\u001b[0m     pa_version_under4p0,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m PY38 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     31\u001b[0m PY39 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/similarity-tickets/notebooks/.venv/lib/python3.12/site-packages/pandas/compat/pyarrow.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     _pa_version \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\n\u001b[1;32m      9\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(_pa_version)\n\u001b[1;32m     10\u001b[0m     pa_version_under1p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import db_dtypes\n",
    "import ftfy\n",
    "import re\n",
    "import six\n",
    "import tiktoken\n",
    "import pandas_gbq\n",
    "from unidecode import unidecode\n",
    "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import json\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "#from asyncio.log import logger\n",
    "from util import transform_sentence\n",
    "from logger import get_logger\n",
    "import psycopg2.extras as extras \n",
    "import hashlib\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "import openai\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ea251-192a-4efa-82da-08eeff0a2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    \n",
    "    def run_select_statement(select_statement: str):\n",
    "        credentials = service_account.Credentials.from_service_account_file('/Users/rodrigomoraes/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/TOTVS LABS/key_SA_GCP/labs-poc-09feb4e7688e.json')\n",
    "\n",
    "        project_id = 'labs-poc'\n",
    "        client = bigquery.Client(credentials= credentials,project=project_id)\n",
    "\n",
    "        try:\n",
    "            # Perform a query.\n",
    "            query_job = client.query(select_statement)  # API request\n",
    "            result = query_job.result()\n",
    "        except Exception as e:\n",
    "            print(f'Fetching resuls from database failed. {e}\\nSelect statement: {select_statement}')\n",
    "            raise\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277bd1c2-49cc-4be2-ab2d-19fe5ceb6b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatabaseService:\n",
    "\n",
    "    def __init__(self):\n",
    "        db_user='tecsupport'\n",
    "        db_password='?Hi((<={}F{nI=jp'\n",
    "        db_database='tecsupport'\n",
    "        db_port='5432'\n",
    "        db_host='34.123.172.21'\n",
    "        self.connection_str = f\"host='{db_host}' port='{db_port}' dbname='{db_database}' user='{db_user}' password='{db_password}'\"\n",
    "\n",
    "    def _get_database_connection(self):\n",
    "        return psycopg2.connect(self.connection_str)\n",
    "\n",
    "    def run_select_statement(self, select_statement: str, vars=None):\n",
    "        #logger = get_logger(__name__)\n",
    "        \n",
    "        try:\n",
    "            conn = self._get_database_connection()\n",
    "            register_vector(conn)\n",
    "        except Exception as e:\n",
    "            print(f'Connecting to database failed. {e}')\n",
    "            return []\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(select_statement, vars=vars)\n",
    "            fields = [field_md[0] for field_md in cursor.description]\n",
    "            result = cursor.fetchall()\n",
    "            result = [dict(zip(fields, row)) for row in result]\n",
    "        except Exception as e:\n",
    "            print(f'Fetching resuls from database failed. {e}\\nSelect statement: {select_statement}')\n",
    "            conn.rollback()\n",
    "            result = []\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def run_dml_statement(self, df: str, table: str, vars=None): \n",
    "        logger = get_logger(__name__)\n",
    "        \n",
    "        try:\n",
    "            conn = self._get_database_connection()\n",
    "            register_vector(conn)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Connecting to database failed. {e}')\n",
    "            return []\n",
    "        \n",
    "        tuples = [tuple(x) for x in df.to_numpy()] \n",
    "  \n",
    "        cols = ','.join(list(df.columns)) \n",
    "        # SQL query to execute \n",
    "        query = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols) \n",
    "        cursor = conn.cursor() \n",
    "        try: \n",
    "            extras.execute_values(cursor, query, tuples) \n",
    "            conn.commit() \n",
    "        except (Exception, psycopg2.DatabaseError) as error: \n",
    "            print(\"Error: %s\" % error) \n",
    "            conn.rollback() \n",
    "            cursor.close() \n",
    "            return 1\n",
    "        \n",
    "        query = \"SELECT COUNT(*) as cnt FROM %s;\" % (table) \n",
    "        cursor.execute(query)\n",
    "        num_records = cursor.fetchone()[0]\n",
    "\n",
    "        print(\"Number of vector records in table: \", num_records,\"\\n\")\n",
    "\n",
    "        # Create an index on the data for faster retrieval\n",
    "\n",
    "        #calculate the index parameters according to best practices\n",
    "        # num_lists = num_records / 1000\n",
    "        # if num_lists < 10:\n",
    "        #     num_lists = 10\n",
    "        # if num_records > 1000000:\n",
    "        #     num_lists = math.sqrt(num_records)\n",
    "\n",
    "        #use the cosine distance measure, which is what we'll later use for querying\n",
    "        # cursor.execute(f'CREATE INDEX ON {table} USING ivfflat (sentence_embedding vector_cosine_ops) WITH (lists = {num_lists});')\n",
    "        # cursor.execute(f'CREATE INDEX idx ON {table} USING hnsw (sentence_embedding vector_l2_ops);')\n",
    "        # conn.commit()\n",
    "\n",
    "        cursor.close()  \n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02422b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_SELECT_COLUMNS = ['ticket_id', 'ticket_comment', 'ticket_sentence_hash', 'module', 'product',\n",
    "                             'sentence_source', 'ticket_status', 'created_at', 'updated_at', 'score', 'first_comment']\n",
    "\n",
    "class DocumentSearchService:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.database_service = DatabaseService()\n",
    "        self.threshold = 65\n",
    "\n",
    "    def ticket_summarization(self, sentence: str, llm):\n",
    "        \"\"\"Returns ticket summarization by comment.\n",
    "\n",
    "        :param sentence: Sentence to summarization.\n",
    "        :param llm: LLMChain object.\n",
    "        \"\"\"\n",
    "\n",
    "        text_splitter = CharacterTextSplitter()\n",
    "        texts = text_splitter.split_text(sentence)\n",
    "        docs = [Document(page_content=t) for t in texts[:4]]\n",
    "        \n",
    "        # prompt_template = \"\"\"Write a concise topic based on the following passages, in Brazilian Portuguese, \n",
    "        #                      disregarding all personal names, date, attach and url of the following:\n",
    "\n",
    "        # Perguntar ao prompt sobre requisicao e dúvida, fornecendo os comentarios, explicar qual a duvida/problema, \n",
    "        # com os demais itens (nomes, url...)\n",
    "        # with a custom prompt\n",
    "        prompt_template = \"\"\"Write a concise summary, in chronological order, in Brazilian Portuguese, \n",
    "                            disregarding all personal names, date, attach and url, of the following:\n",
    "\n",
    "\n",
    "        {text}\n",
    "\n",
    "\n",
    "        CONCISE SUMMARY:\"\"\"\n",
    "        PROMPT = PromptTemplate(template=prompt_template,\n",
    "                                input_variables=[\"text\"])\n",
    "\n",
    "        ## with intermediate steps\n",
    "        chain = load_summarize_chain(llm,\n",
    "                                    chain_type=\"map_reduce\",\n",
    "                                    return_intermediate_steps=True,\n",
    "                                    map_prompt=PROMPT,\n",
    "                                    combine_prompt=PROMPT)\n",
    "\n",
    "        output_summary = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "        wrapped_text = textwrap.fill(output_summary['output_text'],\n",
    "                                    width=100,\n",
    "                                    break_long_words=False,\n",
    "                                    replace_whitespace=False)\n",
    "    \n",
    "        return wrapped_text\n",
    "\n",
    "    def embeddings_search_on_database(self, query_vec: np.array, product: str, module: str,\n",
    "                                      threshold: int, similarity: str, ticket_id: int, batch: bool):\n",
    "\n",
    "        table_name = 'tickets_embeddings'\n",
    "        \n",
    "        if batch == False:\n",
    "            select_statement = f'''SELECT * FROM\n",
    "                                    (\n",
    "                                        SELECT *, 1 - (sentence_embedding {similarity} %s) as score \n",
    "                                        FROM public.{table_name}\n",
    "                                        WHERE (product ~ 'ˆ{product}.*' OR \n",
    "                                                module ~ '{module}')\n",
    "                                            and ticket_id <> {ticket_id}\n",
    "                                    ) as filtered_kb\n",
    "                                WHERE score > {threshold/100};\n",
    "                            '''\n",
    "        else:\n",
    "            select_statement = f'''SELECT * FROM\n",
    "                                    (\n",
    "                                        SELECT te.ticket_id, 1 - (sentence_embedding {similarity} %s) as score,\n",
    "                                        (\n",
    "                                            SELECT\n",
    "                                                expected_id\n",
    "                                            FROM\n",
    "                                                public.tickets_similares\n",
    "                                            WHERE ticket_id = {ticket_id}\n",
    "                                        ) as expected_id \n",
    "                                        FROM public.{table_name} te\n",
    "                                        WHERE te.ticket_id <> {ticket_id}\n",
    "                                    ) as filtered_kb\n",
    "                                WHERE score > {threshold/100};\n",
    "                            '''\n",
    "                                            \n",
    "        result = self.database_service.run_select_statement(select_statement, (query_vec,))\n",
    "        \n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    def find_tickets_for_query(self, query: str, product: str, module: str, k: int, similarity: str, ticket_id: int, batch: bool):\n",
    "        \n",
    "        if batch == False:\n",
    "            # Searching tickets using similarity of OpenAPI embeddings\n",
    "            query_vec =  OpenAIEmbeddings(\n",
    "                openai_api_base=\"https://proxy.dta.totvs.ai/\",\n",
    "                openai_api_key=\"sk-axyZ_tPhqNPbbywhdhhhKQ\",\n",
    "                model=\"text-embedding-3-small\"  \n",
    "                ).embed_query(query)\n",
    "            query_vec = np.array(query_vec)\n",
    "        else:\n",
    "            query_vec = query\n",
    "\n",
    "        results = self.embeddings_search_on_database(query_vec, product, module, self.threshold, similarity, ticket_id, batch)\n",
    "\n",
    "        if results.empty:\n",
    "            return results\n",
    "        \n",
    "        # Getting only results with score higher than threshold\n",
    "        results = results[results[\"score\"] >= self.threshold / 100].copy()\n",
    "\n",
    "        # Ordering results by score\n",
    "        results.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "\n",
    "        # Keeping only the highest rank per ticket'\n",
    "        results.drop_duplicates(subset=['ticket_id'], keep=\"first\", inplace=True)\n",
    "        results = results.head(k)\n",
    "\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96660672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "# Create similar tickets from the sent spreadsheet\n",
    "def similarity_ticket():\n",
    "\n",
    "    select_statement = (\"\"\"\n",
    "                        SELECT\n",
    "                            CAST(ticket_id AS INT64) AS ticket_id,\n",
    "                            expected_id\n",
    "                        FROM\n",
    "                            `labs-poc.custom_data.tickets_similares`\n",
    "                        \"\"\")\n",
    "    \n",
    "    df = LoadData.run_select_statement(select_statement).to_dataframe()\n",
    "\n",
    "    # Grava o resultado dos dados coletados no BQ e faz insert do Dataframe diretamente no Banco Vetorizado\n",
    "    DatabaseService().run_dml_statement(df, 'tickets_similares')\n",
    "\n",
    "    return df\n",
    "\n",
    "def embedding_sentence(result_df, i, llm, embedding):\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Cria os embeddings para inserir os registros.\n",
    "    #for i, row in result_df.iterrows():\n",
    "    sentence = result_df.at[i,'ticket_comment']\n",
    "    summary = DocumentSearchService().ticket_summarization(sentence, llm)\n",
    "\n",
    "    # Cria embeddings para ticket_comment\n",
    "    query_vec = embedding.embed_query(sentence)\n",
    "    query_vec = np.array(query_vec)\n",
    "\n",
    "    # Atualiza os valores da linha atual\n",
    "    hash_concat = str(result_df.at[i,'ticket_id']) + sentence\n",
    "    hash_id = hashlib.md5(hash_concat.encode('utf-8')).hexdigest()\n",
    "    result_df.at[i,'ticket_sentence_hash'] =  hash_id\n",
    "    result_df.at[i,'summary'] = summary\n",
    "    result_df.at[i,'sentence_embedding'] = query_vec\n",
    "    result_df.at[i,'sentence_source'] = 'ticket_comment'\n",
    "\n",
    "    # Grava o resultado dos dados coletados no BQ e faz insert do Dataframe diretamente no Banco Vetorizado\n",
    "    DatabaseService().run_dml_statement(result_df.iloc[[i]], 'tickets_embeddings')\n",
    "\n",
    "    # Cria nova linha com dados do Assunto\n",
    "    ultima_linha = result_df.loc[i]\n",
    "    nova_linha = ultima_linha.copy()\n",
    "    sentence = nova_linha['subject']\n",
    "    hash_concat = str(nova_linha['ticket_id']) + sentence\n",
    "    hash_id = hashlib.md5(hash_concat.encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Cria embeddings para a sentença\n",
    "    query_vec = embedding.embed_query(sentence)\n",
    "    query_vec = np.array(query_vec)\n",
    "\n",
    "    # Atualiza os valores da linha atual\n",
    "    nova_linha['ticket_sentence_hash'] =  hash_id\n",
    "    nova_linha['summary'] = summary\n",
    "    nova_linha['sentence_embedding'] = query_vec\n",
    "    nova_linha['sentence_source'] = 'subject'\n",
    "    result_df = result_df.append(nova_linha, ignore_index=True)\n",
    "\n",
    "    # Grava o resultado dos dados coletados no BQ e faz insert do Dataframe diretamente no Banco Vetorizado\n",
    "    DatabaseService().run_dml_statement(result_df.tail(1), 'tickets_embeddings')\n",
    "\n",
    "    # Cria nova linha com dados do Resumo\n",
    "    sentence = summary\n",
    "    ultima_linha = result_df.loc[i]\n",
    "    nova_linha = ultima_linha.copy()\n",
    "    hash_concat = str(nova_linha['ticket_id']) + sentence\n",
    "    hash_id = hashlib.md5(hash_concat.encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Cria embeddings para a sentença\n",
    "    query_vec = embedding.embed_query(sentence)\n",
    "    query_vec = np.array(query_vec)\n",
    "\n",
    "    nova_linha['ticket_sentence_hash'] =  hash_id\n",
    "    nova_linha['summary'] = summary\n",
    "    nova_linha['sentence_embedding'] = query_vec\n",
    "    nova_linha['sentence_source'] = 'summary'\n",
    "    result_df = result_df.append(nova_linha, ignore_index=True)\n",
    "\n",
    "    # Grava o resultado dos dados coletados no BQ e faz insert do Dataframe diretamente no Banco Vetorizado\n",
    "    DatabaseService().run_dml_statement(result_df.tail(1), 'tickets_embeddings')\n",
    "\n",
    "    # Cria nova linha com dados do primeiro comentario\n",
    "    ultima_linha = result_df.loc[i]\n",
    "    nova_linha = ultima_linha.copy()\n",
    "    sentence = nova_linha['first_comment']\n",
    "    hash_concat = str(nova_linha['ticket_id']) + sentence + summary\n",
    "    hash_id = hashlib.md5(hash_concat.encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Cria embeddings para a sentença\n",
    "    query_vec = embedding.embed_query(sentence)\n",
    "    query_vec = np.array(query_vec)\n",
    "\n",
    "    # Atualiza os valores da linha atual\n",
    "    nova_linha['ticket_sentence_hash'] =  hash_id\n",
    "    nova_linha['summary'] = summary\n",
    "    nova_linha['sentence_embedding'] = query_vec\n",
    "    nova_linha['sentence_source'] = 'first_comment'\n",
    "    result_df = result_df.append(nova_linha, ignore_index=True)\n",
    "\n",
    "    # Grava o resultado dos dados coletados no BQ e faz insert do Dataframe diretamente no Banco Vetorizado\n",
    "    DatabaseService().run_dml_statement(result_df.tail(1), 'tickets_embeddings')\n",
    "\n",
    "    return result_df\n",
    "\n",
    "select_statement = (\"\"\"\n",
    "                    WITH tickets_all AS (\n",
    "                        SELECT\n",
    "                            ticket_id,\n",
    "                            STRING_AGG(ticket_comment, '\\u2561') AS ticket_comment,\n",
    "                            MAX(subject) AS subject,\n",
    "                            '' AS summary,\n",
    "                            '' ticket_sentence_hash,\n",
    "                            MAX(module_name) AS module,\n",
    "                            MAX(product_name) AS product,\n",
    "                            '' AS sentence_source,\n",
    "                            MAX(ticket_status) AS ticket_status,\n",
    "                            '[1,2,3]' AS sentence_embedding,\n",
    "                            MAX(created_at) AS created_at,\n",
    "                            MAX(updated_at) AS updated_at,\n",
    "                        FROM\n",
    "                            `labs-poc`.custom_data.tickets tr\n",
    "                        GROUP BY\n",
    "                            ticket_id\n",
    "                        ),\n",
    "\n",
    "                        first_contact AS (\n",
    "                        SELECT\n",
    "                            ts.ticket_comment AS first_comment,\n",
    "                            ts.ticket_id\n",
    "                        FROM\n",
    "                            `labs-poc`.custom_data.tickets ts\n",
    "                        INNER JOIN\n",
    "                            tickets_all tr\n",
    "                        ON\n",
    "                            ts.ticket_id = tr.ticket_id\n",
    "                        QUALIFY ROW_NUMBER() OVER(PARTITION BY ts.ticket_id ORDER BY ts.comment_created_at) = 1\n",
    "                        )\n",
    "\n",
    "                        SELECT\n",
    "                            ta.*,\n",
    "                            tr.first_comment\n",
    "                        FROM\n",
    "                            tickets_all ta\n",
    "                        INNER JOIN\n",
    "                            first_contact tr\n",
    "                            ON tr.ticket_id = ta.ticket_id\n",
    "\n",
    "                    \"\"\")\n",
    "# Faz a leitura dos daods no BQ\n",
    "df = LoadData.run_select_statement(select_statement).to_dataframe()\n",
    "\n",
    "dados = df\n",
    "\n",
    "# Instancia llm e embedding\n",
    "llm = OpenAI(\n",
    "            openai_api_base=\"https://proxy.dta.totvs.ai/\",\n",
    "            openai_api_key=\"sk-axyZ_tPhqNPbbywhdhhhKQ\",\n",
    "            temperature=0,\n",
    "            model=\"gpt-4o\",\n",
    "            )\n",
    "\n",
    "embedding =  OpenAIEmbeddings(\n",
    "            openai_api_base=\"https://proxy.dta.totvs.ai/\",\n",
    "            openai_api_key=\"sk-axyZ_tPhqNPbbywhdhhhKQ\",\n",
    "            model=\"text-embedding-3-small\"  \n",
    "            )\n",
    "\n",
    "# Usa o joblib para paralelizar o processamento. 10 jobs por vez usando o backend threading.\n",
    "Parallel(n_jobs=10, backend='threading')(delayed(embedding_sentence)(\n",
    "         dados,i, llm, embedding)\n",
    "         for i, row in dados.iterrows())\n",
    "\n",
    "# Gera dados na tabela de tickets similares, estes dados vem da planilha enviada para busca de ticket similares\n",
    "similarity_ticket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7ede8-0515-418b-8ed5-cb904bbcbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticket_id: 19432291\n",
    "# <->: This pgvector query operator calculates the Euclidean (L2) distance between two vectors.\n",
    "# Busca por summary\n",
    "# Tickes com maior similaridade: \n",
    "            # 19369960 = ok\n",
    "            # 19440602\n",
    "            # 19249692 = ok\n",
    "            # 19129354\n",
    "            # 19141108 = ok\n",
    "            # 18827337    \n",
    "            # 18844460 = ok\n",
    "            # 18980613 = ok\n",
    "query = \"\"\"Acessar espelho de ponto 2018 - mensagem-> não utiliza Cartão de Ponto\n",
    "    \"\"\"\n",
    "\n",
    "product = 'Datasul'\n",
    "module = ' '\n",
    "\n",
    "documents_df = DocumentSearchService().find_tickets_for_query(query, product, module, k=20, similarity = '<->')\n",
    "documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132fa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticket_id: 19432291\n",
    "# <=>: This operator computes the cosine similarity between two vectors. Cosine similarity\n",
    "# Busca por summary\n",
    "# Tickes com maior similaridade: \n",
    "            # 19369960 = ok\n",
    "            # 19440602 = ok top 1 (deve haver hit)\n",
    "            # 19249692 = ok top 2\n",
    "            # 19129354\n",
    "            # 19141108 = ok \n",
    "            # 18827337    \n",
    "            # 18844460 = ok\n",
    "            # 18980613 = ok top 3\n",
    "query = \"\"\"Erro ao aplicar index CRM pacote 12.1.2311-2\"\"\"\n",
    "ticket_id = 19432291\n",
    "product = 'Planos - Linha Datasul'\n",
    "module = ' '\n",
    "\n",
    "documents_df = DocumentSearchService().find_tickets_for_query(query, product, module, k=20, similarity='<=>', ticket_id=ticket_id, batch=False)\n",
    "documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc260a4-4871-44e4-9956-84844c115df2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatabaseService' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Connecting to the database\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mDatabaseService\u001b[49m()\u001b[38;5;241m.\u001b[39m_get_database_connection()\n\u001b[1;32m     50\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Execute the query to fetch the embeddings for each ticket_id\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatabaseService' is not defined"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_similarity(df, product, module, sentence, index, pbar):\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    ticket_id = df.at[index, 'ticket_id']\n",
    "    query_vec = np.array(df.at[index, 'sentence_embedding'])\n",
    "\n",
    "    # Find similar tickets using cosine similarity\n",
    "    documents_df = DocumentSearchService().find_tickets_for_query(query_vec, product, module, k=20, similarity='<=>', ticket_id=ticket_id, batch=True)\n",
    "    \n",
    "    if not documents_df.empty:\n",
    "        results_df = documents_df\n",
    "\n",
    "        # Sort the DataFrame by score in descending order\n",
    "        df_sorted = results_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "        # Seleciona os três principais ticket_id\n",
    "        top_1 = df_sorted.index[0]\n",
    "        top_2 = df_sorted.index[1] if len(df_sorted) > 1 else None\n",
    "        top_3 = df_sorted.index[2] if len(df_sorted) > 2 else None\n",
    "\n",
    "        # Additional columns corresponding only to found indexes\n",
    "        results_df['Top 1'] = results_df.index == top_1\n",
    "        results_df['Top 2'] = results_df.index == top_2 if top_2 else False\n",
    "        results_df['Top 3'] = results_df.index == top_3 if top_3 else False\n",
    "\n",
    "        # Add top ticket IDs to the DataFrame\n",
    "        results_df['sentence'] = sentence\n",
    "        results_df['target'] = ticket_id\n",
    "\n",
    "        # Sort the final DataFrame by score\n",
    "        df_sorted_final = results_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "        return df_sorted_final\n",
    "\n",
    "    return pd.DataFrame()  # Return an empty DataFrame if no results are found\n",
    "\n",
    "# Ticket IDs to compare in embeddings\n",
    "ticket_ids = [19432291, 19528296, 15893702, 18858827]\n",
    "product = 'Datasul'\n",
    "module = ''\n",
    "sentence = 'subject'\n",
    "\n",
    "# Connecting to the database\n",
    "conn = DatabaseService()._get_database_connection()\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the query to fetch the embeddings for each ticket_id\n",
    "ticket_inputs = []\n",
    "cur.execute(f\"\"\"\n",
    "            select te.ticket_id, sentence_embedding \n",
    "            from tickets_embeddings te\n",
    "            INNER JOIN tickets_similares ts\n",
    "                ON ts.ticket_id = te.ticket_id\n",
    "            where sentence_source = '{sentence}'\n",
    "            \"\"\")\n",
    "ticket_inputs = cur.fetchall()\n",
    "#ticket_inputs.extend(ticket_input)\n",
    "\n",
    "dados = pd.DataFrame(ticket_inputs, columns=['ticket_id', 'sentence_embedding'])\n",
    "\n",
    "# Close the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Initialize the progress bar\n",
    "with tqdm(total=len(dados), desc=\"Processing\") as pbar:\n",
    "    # Define a function to wrap find_similarity and update the progress bar\n",
    "    def process_row(index):\n",
    "        result = find_similarity(dados, product, module, sentence, index, pbar)\n",
    "        pbar.update(1)\n",
    "        return result\n",
    "\n",
    "    # Use Parallel to process each row in parallel\n",
    "    results = Parallel(n_jobs=-1, backend='threading')(delayed(process_row)(i) for i in range(len(dados)))\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat(results, ignore_index=True)\n",
    "group_df = pd.concat(results, ignore_index=True) \n",
    "\n",
    "# Function to check if the ticket_id is in the expected_id\n",
    "def check_found(row):\n",
    "    expected_ids = [int(x.strip()) for x in row['expected_id'].split(',') if x.strip().isdigit()]\n",
    "    return row['ticket_id'] in expected_ids\n",
    "\n",
    "# Function to check if the expected_id is in the ticket_id summarization found and not_found\n",
    "def check_expected(row):\n",
    "    expected_ids = [int(x.strip()) for x in row['expected_id'].split(',') if x.strip().isdigit()]\n",
    "    found_count = sum(1 for eid in expected_ids if eid in all_ticket_ids)\n",
    "    not_found_count = sum(1 for eid in expected_ids if eid not in all_ticket_ids)\n",
    "    return found_count, not_found_count\n",
    "\n",
    "# Apply function and create found and not found columns\n",
    "final_df['found'] = final_df.apply(lambda row: 1 if check_found(row) else 0, axis=1)\n",
    "final_df['not_found'] = final_df.apply(lambda row: 0 if check_found(row) else 1, axis=1)\n",
    "\n",
    "# Export the final DataFrame to Excel\n",
    "final_df.to_excel('final_results.xlsx', index=False)\n",
    "\n",
    "# List of all ticket_id for reference\n",
    "all_ticket_ids = set(group_df['ticket_id'])\n",
    "\n",
    "# Apply function and create found and not found columns\n",
    "group_df[['found', 'not_found']] = group_df.apply(lambda row: pd.Series(check_expected(row)), axis=1)\n",
    "\n",
    "# Keep only the necessary columns\n",
    "group_df = group_df[['expected_id', 'sentence', 'target', 'found', 'not_found']]\n",
    "\n",
    "# Group by target and aggregate the results\n",
    "df_grouped = group_df.groupby('target').agg({\n",
    "    'expected_id': 'first',   # or ' '.join para concatenar\n",
    "    'sentence': 'first',\n",
    "    'found': 'first',\n",
    "    'not_found': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Add a column accuracy\n",
    "df_grouped['accuracy'] = df_grouped.apply(\n",
    "    lambda row: (row['found'] / (row['found'] + row['not_found'])) if (row['found'] + row['not_found']) > 0 else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Export the final DataFrame to Excel\n",
    "df_grouped.to_excel('final_results_grouped.xlsx', index=False)\n",
    "\n",
    "# # Print the sorted DataFrame\n",
    "# print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eedcc4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimilarity_finder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityFinder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create an instance of SearchService\u001b[39;00m\n\u001b[1;32m      4\u001b[0m find_similarity \u001b[38;5;241m=\u001b[39m SimilarityFinder()\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/similarity-tickets/notebooks/similarity_finder.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/similarity-tickets/notebooks/.venv/lib/python3.12/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hard_dependencies, dependency, missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     np_version_under1p18 \u001b[38;5;28;01mas\u001b[39;00m _np_version_under1p18,\n\u001b[1;32m     24\u001b[0m     is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/similarity-tickets/notebooks/.venv/lib/python3.12/site-packages/pandas/compat/__init__.py:23\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     is_numpy_dev,\n\u001b[1;32m     17\u001b[0m     np_array_datetime64_compat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     np_version_under1p20,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     pa_version_under1p0,\n\u001b[1;32m     25\u001b[0m     pa_version_under2p0,\n\u001b[1;32m     26\u001b[0m     pa_version_under3p0,\n\u001b[1;32m     27\u001b[0m     pa_version_under4p0,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m PY38 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     31\u001b[0m PY39 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-rg.moraes@totvs.com.br/My Drive/similarity-tickets/notebooks/.venv/lib/python3.12/site-packages/pandas/compat/pyarrow.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     _pa_version \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\n\u001b[1;32m      9\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(_pa_version)\n\u001b[1;32m     10\u001b[0m     pa_version_under1p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "from similarity_finder import SimilarityFinder\n",
    "\n",
    "# Create an instance of SearchService\n",
    "find_similarity = SimilarityFinder()\n",
    "\n",
    "# Call the accuracy_metrics method with the correct arguments\n",
    "product = 'Datasul'\n",
    "module = ''\n",
    "sentence = 'subject'\n",
    "\n",
    "# Ensure you pass exactly three arguments besides 'self'\n",
    "df_grouped = find_similarity.process_data(product, module, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will display a bar chart of scores for each ticket_id, allowing you to visualize \n",
    "#how similar each document is to the query embedding.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the scores with top positions\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(df_sorted_final.index, df_sorted_final['score'], color='skyblue')\n",
    "# plt.scatter(df_sorted_final[df_sorted_final['Top 1']].index, df_sorted_final[df_sorted_final['Top 1']]['score'], color='red', label='Top 1')\n",
    "# plt.scatter(df_sorted_final[df_sorted_final['Top 2']].index, df_sorted_final[df_sorted_final['Top 2']]['score'], color='green', label='Top 2')\n",
    "# plt.scatter(df_sorted_final[df_sorted_final['Top 3']].index, df_sorted_final[df_sorted_final['Top 3']]['score'], color='orange', label='Top 3')\n",
    "# plt.xlabel('Ticket ID')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Scores of Tickets with Top Positions')\n",
    "# plt.legend()\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Prepare data for visualization\n",
    "#ids, scores = zip(*results)\n",
    "\n",
    "# Select Top X documents\n",
    "top_x = 5\n",
    "top_x_df = final_df.head(top_x)\n",
    "top_x_df = top_x_df.sort_values(by='score', ascending=True)\n",
    "print(top_x_df)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='score', y='ticket_id', data=top_x_df, palette='viridis', orient='h')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('Ticket ID')\n",
    "plt.title(f'Top {top_x} Documents by score')\n",
    "plt.show()\n",
    "\n",
    "# Plot the scores\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# #sns.barplot(x=list(ids), y=list(scores), color='skyblue')\n",
    "# sns.barplot(x='ticket_id', y='score', data=df_sorted_final, color='skyblue')\n",
    "# plt.xlabel('Ticket ID')\n",
    "# plt.ylabel('score')\n",
    "# plt.title('scores of Embeddings')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77ae02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
